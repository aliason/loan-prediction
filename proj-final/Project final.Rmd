---
title: "Loan Prediction Machine Learning Project"
subtitle: "PSTAT 131 Final Project" 

author: "Alice Zhang"
date: "2023-03-24"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
    df_print: paged
    theme: sandstone
    highlight: tango
    

---

```{r setup, include=FALSE}
library(knitr)   # to help with the knitting process
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)  # for modeling functions 
library(corrplot)
library(ggplot2)   # for most of our visualizations
library(ggthemes)
library(corrr)
library(dplyr)
library(plyr)
library(yardstick)  # metrics / metric set 
library(gridExtra)
library(naniar) # to assess missing data patterns
library(pROC)  # analyze ROC curves 
library(finalfit)
library(MASS)    # to assist with the markdown processes
library(dplyr)     # for basic r functions
library(discrim)  # discriminant analysis
library(klaR) # for naive bayes
library(naniar)  # visualize missing data 
library(kableExtra)  
library(themis) # for upsampling
tidymodels_prefer()
options(scipen=999)  # remove scientific notation
```

# Introduction

The aim of this project is to develop a machine learning model that predicts the loan status of customers based on information provided in their application profile. It is a binary classification problem (a binary response) in which we predict whether a loan would be approved or not. I will be using open-source data pulled from [Kaggle](https://www.kaggle.com/datasets/vikasukani/loan-eligible-dataset) (pulled from an [Analytics Vidhya Hackathon](https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/#ProblemStatement) and implementing multiple techniques to yield the most accurate model for the problem. 


## Problem Statement 
Loans are a necessity of the modern world, supporting consumption, economic growth, and business operations. Many types loans exist for different purposes across various stages of life, among which are home loans, which we intend to tackle in this problem. 

Dream Housing Finance company deals in all home loans. They have a presence across all urban, semi-urban and rural areas. Customers can apply for a home loan after the company validates their eligibility. The company wants to automate the loan eligibility process (real-time) based on customer detail provided in their application. The company wants to identify customer segments that are eligible for loan amounts so that they can specifically target these customers.

Loan prediction is a very common real-life problem that every retail bank faces at least once; automating this process could save time, resources, and money. There is, however, an unambiguously bias in lending. As such, we seek to examine the factors most predictive of loan status and fit multiple models to automate loan eligibility. 

```{r echo=FALSE, out.width = "100%", fig.align = "center"}
knitr::include_graphics("~/Desktop/School/PSTAT/PSTAT 131/proj-final/images/image1.jpg")
```


## Dataset description 
The data files provided consists of a training set (train.csv) and test set (test.csv), which contains similar data-points as train except for the loan status to be predicted. The training set consists of 614 observations on 13 variables (8 categorical and 5 numeric); the testing consists of 367 observations on 12. 

Since this project employs supervised learning methods, I intend to use only the training set. The dataset will be split into 70% training and 30% testing, using observed values to evaluate predictive accuracy. 


## Project outline 
First, I will load the data, perform initial data manipulation and cleaning, and address missing values. Next, I will perform exploratory data analysis, employing visualization and inferential techniques to identify trends, patterns, and relationships. After examining the data, I will perform some final tidying before setting up the models. I will split the train.csv into a train and test set (70/30), build a recipe, and create validation sets to generate multiple estimations of the test error rate. We then fit 6 supervised learning models (logistic, LDA, QDA, elastic net, KNN, and pruned decision trees) before assessing their performance using several evaluation metrics. From there, we will select the best model and fit it to our testing data.


# Exploratory Data Analysis
In this section, we will first load the data, perform some data tidying and transformation, examine missing values, and then perform some preliminary analysis on the dataset. 


## Loading and exploring the data
First, let's take a look at our data. 
```{r, class.source = "fold-show", message=FALSE, warning=FALSE} 
loan_ds <- read.csv("~/Desktop/School/PSTAT/PSTAT 131/proj-final/project_data/train.csv")
str(loan_ds)
```

 A few caveats: 
 
  * `Dependents`, a numeric variable, is encoded as a factor. 
  * `Credit_History`, a categorical variable, is encoded as numeric. 
  * `ApplicantIncome` and `CoapplicantIncome` are given as monthly (instead of annual) figures in dollar amounts, while `LoanAmount` is a lump-sum given in terms of thousands. 

\

Now, let's look at missing values:
```{r}
colSums(is.na(loan_ds))  
```

We can see that there are NULLS in `LoanAmount`, `Loan_Amount_term` and `Credit_History`. It is important to note however that `is.na()` does not detect blank cells in character variables. Thus we need to employ a different methodology to identify empty strings.

```{r}
sapply(loan_ds,function(x) table(as.character(x) =="")["TRUE"])
```
Indeed, we have blank entries in `Gender`, `Married`, `Dependents`, and `Self-employed`. We will first transform these blanks into NA's for easy identification, and then determine what to do about them.



## Tidying the data

Transform empty strings in to "NA". 
```{r, class.source = "fold-show"}
loan_ds <- read.csv(file="~/Desktop/School/PSTAT/PSTAT 131/proj-final/project_data/train.csv",
                    header=TRUE,na.strings = c("","NA")) 
```

Scale `ApplicantIncome` and `CoapplicantIncome` to match the format of `LoanAmount`.  
```{r, class.source = "fold-show"}
loan_ds$ApplicantIncome <- (loan_ds$ApplicantIncome*12)/1000
loan_ds$CoapplicantIncome <- (loan_ds$CoapplicantIncome*12)/1000
  # scale into annual figures
  # convert them into the same units as LoanAmount (in terms of thousands)
```

Because `Loan_ID` is unique and not relevant to our analysis, we will remove it. 
```{r, class.source = "fold-show"}
loan_ds <- loan_ds[,-1]; colnames(loan_ds)
```


### Transform variables 
For ease of analysis, we will convert categorical variables into factors and convert `Dependents` into a numeric variable. 
```{r, class.source = "fold-show"}
# convert factors
loan_ds$Gender <- factor(loan_ds$Gender, levels = c("Male","Female"))
loan_ds$Married <- factor(loan_ds$Married, levels = c("Yes","No"))
loan_ds$Education <- factor(loan_ds$Education, levels = c("Graduate","Not Graduate"))
loan_ds$Self_Employed <- factor(loan_ds$Self_Employed, levels = c("Yes","No"))
loan_ds$Property_Area <- factor(loan_ds$Property_Area, levels = c("Rural","Semiurban","Urban"))
loan_ds$Loan_Status <- factor(loan_ds$Loan_Status, levels = c("Y","N"), labels = c("Yes","No")) 

# convert credit history into factor
loan_ds$Credit_History <- factor(loan_ds$Credit_History, levels = c(1,0), labels = c("Yes","No"))  

# convert Dependents into numeric 
loan_ds$Dependents <- recode(loan_ds$Dependents,"3+"="3") %>%
  as.integer(loan_ds$Dependents)
```



### Missing data 

Let's start by visualizing the missing data.
```{r, class.source = "show"}
vis_miss(loan_ds) # visualize missing data
```

It looks like the majority of our variables contain missing values. Let's produce a summary of the missing values to view the percentage of missingness in each variable, as well as a cumulative sum. 
```{r}
loan_ds %>%
  miss_var_summary(add_cumsum = TRUE)
```

There are 149 missing values in our dataset; with `Credit_History` comprising the largest proportion. The simplest solution is to remove variables with a lot of missing values. However, removing variables at this early of a stage is inappropriate since we do not yet know which variables will be significant. As a general rule, we do not want to remove more than 10% of the overall dataset. We will likely need to perform imputation at a later step. 

Let's continue exploring our data and return to this issue later. 



## Variable description
The dataset consists of the following variables: 

* `Loan_ID` : Unique Loan ID.
* `Gender` : Male / Female. 
* `Married` : Whether the applicant is married (Yes/No) 
* `Dependents` : The number of dependents (1,2,3) an applicant has. 
* `Education` : An applicant's education level (Graduate/Under Graduate)
* `Self_Employed` : Whether the applicant is self-employed (Yes/No)
* `ApplicantIncome` : An applicant's annual income (in thousands of dollars). 
* `CoapplicantIncome` : A coapplicant's annual income (in thousands of dollars)
* `LoanAmount` : The loan amount requested by an applicant (in thousands of dollars). 
* `Loan_Amount_Term` : The term of the loan in months. 
* `Credit_History` : Does the applicant's credit history meet the bank's requirements (Yes/No)? 
* `Property_Area` : An applicant's area of residence (Urban/Semi Urban/Rural). 
* `Loan_Status` : Whether the loan was approved (Yes/No). This is the target variable.




## Visual EDA 

This section consists of data exploration and visualization before running our predictive models. We will first look at the response, generate a correlation matrix, and then examine the independent variables one by one to identify potential relationships (and confounds). 

### Loan status 
First, we will look at the distribution of the response by creating a barplot.
```{r}
loan_ds %>% 
  ggplot(aes(x = Loan_Status)) +
  geom_bar() + 
  theme_grey()
```

```{r}
loan_ds %>%
  select(Loan_Status) %>%
  table() %>%
  prop.table() 
```

Approximately 69% of applicants were approved for loan while 31% were rejected. This imbalance in factor levels may make it difficult for the model to learn to predict `Loan_Status` accurately, so we'll need to  *upsample* or *downsample* the data at a later step. 



### Correlation plot
We first create a heatmap of the numeric variables to get an idea of their relationship. The `corrplot()` function generates a correlation matrix, where the main diagonal represents the variance of each variable while the other cells represent covariances The color legend indicates the strength and direction of the relationship for each pair. The plot below shows a moderate, positive correlation between `LoanAmount` and `ApplicantIncome`, and very little correlation (+/- 0.20) among the other predictors.


```{r, echo=FALSE, messages=FALSE, warnings=FALSE}
loan_ds %>%
  select(where(is.numeric)) %>%
  na.omit() %>%   # remember to omit na, or else you'll get ? marks in the grids
  cor() %>%
  corrplot()
```


Below, we can see that `LoanAmount` and `ApplicantIncome` have a correlation coefficient of 0.57. Keep this in mind as we explore the dataset. 
```{r}
loan_ds %>%
  select(where(is.numeric)) %>%
  na.omit() %>%  
  cor() %>%
  corrplot(method="number")
```


Now, we will analyze our predictors one-by-one to examine their distribution and relationship with the response. 



### Loan Amount

We see that loan amount is right (positively) skewed, with most values between 0-400 thousand. As a result, extremely large outliers may pull the mean upwards. The mean loan amount requested by approved applicants and rejected applicants is about the same. However, there is more variation among the rejected applicants. 
```{r}
require(gridExtra)

plot1 <- loan_ds %>%
  na.omit(LoanAmount) %>%
  ggplot(aes(x=LoanAmount)) + 
  geom_histogram(bins=40) +
  theme_grey()

plot2 <- loan_ds %>%
  na.omit(LoanAmount) %>%
  ggplot(aes(Loan_Status, LoanAmount)) + 
  geom_boxplot(na.rm=T) +
  geom_jitter(alpha = 0.1) +
  theme_grey()

grid.arrange(plot1, plot2, ncol=2)
```


Recall that loan amount is positively correlated with applicant income. The first plot shows a slight linear trend between the two variables, especially in the lower amounts on the left. The second plot shows a mix of approved and rejected applicants in each income range. Though it looks like higher income applicants do tend to apply for higher loans, it is difficult to determine whether having a higher income improves their chances of getting approved. We should take a closer look at Applicant Income. 
```{r, echo=FALSE}
plot1 <- loan_ds %>%
  na.omit(LoanAmount) %>% 
  ggplot(aes(x=LoanAmount, y=ApplicantIncome)) +
  geom_point() 

plot2 <- loan_ds %>%
  na.omit(LoanAmount) %>% 
  ggplot(aes(x=LoanAmount, y=ApplicantIncome, fill=Loan_Status,color=Loan_Status)) +
  geom_point() +
  theme_grey()

# we should take a closer look at applicant income
grid.arrange(plot1, plot2, ncol=2)
```





### Applicant Income 

We can see from the plot that applicant incomes are right skewed, with most values between 0-300 thousand. Even with the extremely high outliers omitted, there's a disrepency between the proportions of the income ranges. Average incomes among approved and rejected applicants are about the same. 
```{r}
plot1 <- loan_ds %>%
  filter(ApplicantIncome < 500) %>%  
    # omit high outliers for ease of visualization
  ggplot(aes(x=ApplicantIncome)) + 
  geom_histogram(fill="bisque",color="white",alpha=0.7, bins=20) + 
  geom_density() +
  geom_rug() + 
  labs(x = "applicant income") +
  theme_minimal()

plot2 <- loan_ds %>%
  filter(ApplicantIncome < 500) %>%  
  ggplot(aes(y=ApplicantIncome,x=Loan_Status, color=Loan_Status))+
  geom_boxplot() +
  theme_grey()

grid.arrange(plot1, plot2, ncol=2)
```

Upon inspecting the outliers (`ApplicantIncome` > 500), we find that: 1) 2 out of 3 applicants have dependents, 2) 3 out of 3 are graduates, and 3) 3 out of 3 have no coapplicant. Assuming coapplicants are spouses, those with dependents are likely single fathers. The only rejected applicant 1) lives in a rural area, and 2) has bad credit history. From these observations, we may infer that dependents don’t seem to affect loan eligibility much, neither does loan amount or term. Let's keep this in mind. 
```{r}
loan_ds %>%
  filter(ApplicantIncome > 500)
```


Let's take a closer look at proportions. Applicants with incomes in the 400-500 range have the highest approval rate of 75%. So, higher income do somewhat translate to a higher approval rate. But since there are so few data points in the (400,500] range, this may just be random variation.
```{r, echo=FALSE, message=FALSE, warning=FALSE}
loan_ds %>%
  filter(ApplicantIncome < 500) %>%  
  dplyr::mutate(bin = cut(ApplicantIncome, breaks=c(0, 100, 200, 300,400,500))) %>%
  group_by(bin, Loan_Status) %>% 
  dplyr:: summarise(n=n()) %>%
  dplyr::mutate(freq = prop.table(n))  
```





### Coapplicant Income

Let's start by taking a look at its density, grouped by loan status. Both plots show that coapplicant incomes are right skewed, with a mix of high and low incomes in each loan status category. Average coapplicant income for approved applicants is slightly higher than that of rejected applicants. 

```{r, echo=FALSE}
loan_ds %>%
  ggplot(aes(x=CoapplicantIncome)) + 
  geom_histogram(fill="bisque",color="white",alpha=0.7, bins=20) + 
  geom_density() +
  geom_rug() + 
  labs(x = "Coapplicant Income") +  # stop here for density alone (w/o groups)
  facet_wrap(~Loan_Status)  +   
  theme_minimal() # coapp income is also right skewed, w/ most values >100k 
```

```{r}
loan_ds %>%
  ggplot(aes(x=Loan_Status, y=CoapplicantIncome, color=Loan_Status)) + 
  geom_boxplot()
```


An interesting finding is that a lot of coapplicant incomes are 0 (ie., no coapplicant) in both the Yes and No category. I was confident that having a high coapplicnat income would boost approval rates.

Let's explore this further. The table below shows that 273 out of 614 records have a coapplicant income of 0, or about 44%. This is pretty large given the size of our dataset.

```{r, message=FALSE, warning=FALSE}
loan_ds %>%
  dplyr :: count(CoapplicantIncome == 0) 
```


A natural question we may ask is how the *presence* of a coapplicant alone (not the numerical value of their income) affects the chances that a given applicant will be approved. The variable `has_coapp` has the value of FALSE if coapplicant income is 0, and TRUE otherwise. From the contingency table, we see that about 72% of applicants with a coapplicants get approved, while only ~65% of applicants without a coapplicant get approved. That's a 7% difference!

```{r, message=FALSE, warning=FALSE} 
loan_ds %>%
  dplyr:: mutate(has_coapp = if_else(CoapplicantIncome != 0,TRUE,FALSE)) %>%
  group_by(has_coapp, Loan_Status) %>% 
  dplyr:: summarise(n=n()) %>%
  dplyr::mutate(freq = prop.table(n))  
# on average, ~72% of applicants w/ a coapplicant were approved for a loan
# while only ~65% of applicants w/o a coapplicant were approved.
```

Perhaps the presense of a coapplicant is more predictive of loan status than the numerical value of their income. We should consider transforming `CoapplicantIncome` into a factor.




### Loan amount term

Loan amout term is the term of the loan in months. From the plot below, we see that it has a left skew. This means that its mean (342 months ie., 28.5 years) is lower than its median and mode (both; 360 months ie., 30 years). Because there are so few points on the lower end, the mode is more representative of the center.
```{r}
loan_ds %>%
  na.omit(Loan_Amount_Term) %>%
  ggplot(aes(x=Loan_Amount_Term)) + 
  geom_bar() +
  theme_grey()
# mfv 360 
```


We now look at `Loan_Amount_Term` in relation to `Loan_Status` Applicants requesting short-term loans seem more likely to qualify, on average, but we do see a high approval rate for 360. We should also keep in mind that ~85% of loans have a term of 360, however, so the data may be underrepresenting applicants requesting alternative loan terms.
```{r}
loan_ds %>%
  na.omit(Loan_Amount_Term) %>%  
  ggplot(aes(x=Loan_Amount_Term, fill=Loan_Status)) + 
  geom_bar(position="fill")
```



### Dependents

Dependents is right skewed; most applicants have no dependents. Approval rates are relatively similar across each number of dependents, with the highest approval rate for 2. There is no clear pattern; this indicates that an applicant's `Dependents` may not be influential in determining their `Loan_Status`.
```{r}
plot1<-loan_ds %>%
  na.omit(Dependents) %>%  # should be able to impute this later
  ggplot(aes(x=Dependents)) +
  geom_bar()
# most applicants have no dependents

# dependents vs. loan status
plot2<-loan_ds %>%
  na.omit(Dependents) %>%  
  ggplot(aes(x=Dependents, fill=Loan_Status)) + 
  geom_bar(position="fill")
# relatively similar likelihood of approval for each # of dependents

grid.arrange(plot1,plot2,ncol=2)
```

Do applicants with dependents request a larger loan than those without? Indeed, we can see from the boxplots that individuals with dependents do, on average, request a larger loan amount! 
```{r, echo=FALSE}
loan_ds %>%
  na.omit(Dependents,LoanAmount) %>%  
  ggplot(aes(x=LoanAmount, y=Dependents, group=Dependents, fill=Dependents)) + 
  geom_boxplot()
# yes! 
# individuals with dependents, on average, request a larger loan amount
```




### Gender

One thing to note is that there are more male applicants than female applicants (81% vs 19%); thus females may be under represented. A natural question to ask is whether there is bias. From the plots below, we can see that females are indeed less likely (about 8%) to be approved for a loan than males.

```{r, message=FALSE, warning=FALSE}
prop.table(table(loan_ds$Gender))
# much more males than females.. unrepresentative? 

# is there bias in the selection process? 
loan_ds %>%
  na.omit(Gender) %>%  # should be able to impute this later
  ggplot(aes(x=Gender, fill=Loan_Status)) + 
  geom_bar(position="fill")

# 2-way contingency table 
loan_ds %>%
  na.omit(Gender) %>%
  group_by(Gender, Loan_Status) %>% 
  dplyr:: summarise(n=n()) %>%
  dplyr::mutate(freq = prop.table(n))  
# slight bias; females 8% less likely to be approved than males
```



### Married

Surprisingly, given that most applicants have no dependents, married individuals comprise a majority of our dataset (~60%). Nonetheless, a 60-40 ratio offers a good contrast. Married individuals 10% more likely to be approved for a loan. Given the size of our dataset, a 10% difference is pretty significant! 
```{r, message=FALSE, warning=FALSE}
# distribution
prop.table(table(loan_ds$Married))

# are married individuals more likely to be approved?
loan_ds %>%
  na.omit(Married) %>%
  ggplot(aes(x=Married,fill=Loan_Status)) +
  geom_bar(position="fill")

# 2-way contingency table 
loan_ds %>%
  na.omit(Married) %>%
  group_by(Married, Loan_Status) %>% 
  dplyr:: summarise(n=n()) %>%
  dplyr::mutate(freq = prop.table(n))  
# given the size of our dataset, 10% is pretty large! 
```



### Education
Education denotes an applicant's educational attainment; Graduate or Not Graduate. This may be an oversimplification of the possible levels of education that can be attained; however, a simple encoding is just what we need. Our dataset is comprised of ~80% graduates, which makes sense due to educational loans. An 80-20 ratio, however, is definitely unbalanced. From the barplots, we can see that graduates are more likely to get approved (8% more). 
```{r, message=FALSE, warning=FALSE}
prop.table(table(loan_ds$Education))
# our data is comprised of ~80% graduates!
# makes sense b/c educational loans, etc. 

loan_ds %>%
  na.omit(Education) %>%
  ggplot(aes(x=Education,fill=Loan_Status)) +
  geom_bar(position="fill")
# graduates are slightly more likely to get approved 

# 2-way contingency table 
loan_ds %>%
  na.omit(Education) %>%
  group_by(Education, Loan_Status) %>% 
  dplyr:: summarise(n=n()) %>%
  dplyr::mutate(freq = prop.table(n))  
# graduates are about 8% more likely 
```



### Self employed
Self-employed is a categorical variable that indicates if an applicant is self-employed. Only 14% of applicants in the dataset are self-employed. There is no significant difference in the approval rates between self-employed and non self-employed individuals; a slight difference of 4%, with non self-employed individuals having the higher rate.
```{r,message=FALSE, warning=FALSE}
prop.table(table(loan_ds$Self_Employed))

loan_ds %>%
  na.omit(Self_Employed) %>%
  ggplot(aes(x=Self_Employed,fill=Loan_Status)) +
  geom_bar(position="fill")

# 2-way contingency table 
loan_ds %>%
  na.omit(Self_Employed) %>%
  group_by(Self_Employed, Loan_Status) %>% 
  dplyr:: summarise(n=n()) %>%
  dplyr::mutate(freq = prop.table(n))  
# slight difference; not self-empoyed 4% more likely to be approved
```



### Credit History

Credit history is a categorical variable that indicates if an applicant's credit history satisfies the bank's requirements (ie., if they have good credit history). Most applicants do have good credit history (~85%; unbalanced).  There may be some selection bias, though, since individuals with good credit history may be more inclined to apply in the first place. Unsurprisingly, credit history turns out to be a **very** important predictor for loan status, given that nearly 80% of applicants with good credit history get approved, whereas only 10% of applicants with bad credit history do. 

```{r, message=FALSE, warning=FALSE}
prop.table(table(loan_ds$Credit_History))
# most applicants have good credit history! 

loan_ds %>%
  na.omit(Credit_History) %>%
  ggplot(aes(x=Credit_History,fill=Loan_Status)) +
  geom_bar(position="fill")

# 2-way contingency table 
loan_ds %>%
  na.omit(Credit_History) %>%
  group_by(Credit_History, Loan_Status) %>% 
  dplyr:: summarise(n=n()) %>%
  dplyr::mutate(freq = prop.table(n))  
# *** VERY important predictor!!
# about 70% more likely to be approved if yes! 
# only 10% of applicants w/ bad credit history get approved
```




### Property Area
Property area is a categorical variable that indicates the area in which an applicant resides (Urban, Semi Urban, Rural). We have a pretty good mix of applicants from all 3 areas. Semiurban has the highest approval rate, then urban, then rural. With respect to other predictors, we can see that married individuals tend to prefer semiurban areas over urban or rural areas. This is a pretty insightful finding because, as we recall, married individuals had a 10% higher approval rate than non-married individuals. Coupled with a higher approval rate for those with coapplicants, we may just find our target demographic!

```{r, message=FALSE, warning=FALSE}
prop.table(table(loan_ds$Property_Area))
# good mix of applicants from all 3 areas

loan_ds %>%
  na.omit(Property_Area) %>%
  ggplot(aes(x=Property_Area,fill=Loan_Status)) +
  geom_bar(position="fill")

# 2-way contingency table 
loan_ds %>%
  na.omit(Property_Area) %>%
  group_by(Property_Area, Loan_Status) %>% 
  dplyr:: summarise(n=n()) %>%
  dplyr::mutate(freq = prop.table(n))  

# is property area related to any other predictors?
loan_ds %>%
  na.omit(Property_Area, Loan_Status, Married) %>%
  dplyr:: mutate(has_coapp = if_else(CoapplicantIncome != 0,TRUE,FALSE)) %>%
  ggplot(aes(x=Property_Area,fill=Married)) +
  geom_bar(position="dodge") +
  facet_wrap(~has_coapp)
```




## Final tidying

Now that we've explored the dataset, we will need to fix some errors before continuing with our analysis. Let's review these issues:
  
  * There are missing values in `Credit_History`, `Self_Employed`, `LoanAmount`, `Dependents`, `Loan_Amount_Term`, `Gender`, and `Married`. Based on the type of variable, we will determine which method to use. 
  * Looking at the distributions of the data, we see that `ApplicantIncome` and `LoanAmount` have some extreme (high) outliers. 



### MICE 
Missingness exists in both numerical and categorical data. Therefore, we will be using the `mice` package. The MICE (Multivariate Imputation by Chained Equations) algorithm imputes missing values with plausible data values inferred from other variables in the dataset. 

```{r, class.source = "fold-show", message=FALSE, warning=FALSE}
# install and load 
# install.packages("mice")
library(mice)
```

From the missing data table below, we see that the first two variables are missing a large proportion of its values, while the latter five are missing some. 
```{r, message=FALSE, warning=FALSE}
loan_ds %>%
  miss_var_summary()
```


Now, we call the `mice` package. The argument `m` indicates the number of multiple imputations; the standard is m=5. The `method` argument specifies the imputation method applied to all variables in the dataset; a separate method can also be specified for each variable. 

We can control the `defaultMethod` used for 1) numeric data, 2) categorical data with 2 levels, 3) categorical data with >2 unordered levels, and 4) factor data with >2 ordered levels. I will choose predictive mean matching for numeric data, logistic regression for 2-level factors, linear discriminant analysis for unordered factor data, and proportional odds for ordered factor data.


```{r, class.source = "fold-show", message=FALSE, warning=FALSE, results=FALSE}
imp <- mice(loan_ds, m=5, defautMethod = c("pmm","logreg", "lda", "polr"))
```


Here, we can see the actual imputations for `Dependents`: 
```{r, class.source = "fold-show",} 
imp$imp$Dependents
```


Now let's merge the imputed data into our original dataset via the `complete()` function.
```{r, class.source = "fold-show"} 
loan_ds <- complete(imp,5)  # I chose the 5th round of data imputation
```

Check missing data again, we note that there is no missing data after the imputation:
```{r, class.source = "fold-show", message=FALSE, warning=FALSE}
loan_ds %>%
  miss_var_summary()
```




### Outliers

Outliers can be tricky. It's hard to determine if they are data entry errors, sampling errors, or natural variation in our data. If we decide to remove records, however, it may result in information loss. We will assume that the missing values are systematic until proven otherwise. 


Looking at `LoanAmount`, we see that the "extreme" values are somewhat plausible. Some customers may want to apply for a loan as high as 650 thousand.

```{r, message=FALSE, warning=FALSE}
zscore <- (abs(loan_ds$LoanAmount-mean(loan_ds$LoanAmount, na.rm=T))/sd(loan_ds$LoanAmount, na.rm=T))
loan_ds$LoanAmount[which(zscore > 3)]
```

Since we have a positive skew, we will perform a log transformation to normalize the data. Now the data looks closer to normal and the effect of extreme outliers are significantly smaller.
```{r,class.source = "fold-show"} 
loan_ds$LogLoanAmount <- log(loan_ds$LoanAmount)
```

```{r}
plot1 <- loan_ds %>% 
  ggplot(aes(x=LoanAmount)) +
  geom_histogram(bins=20) +
  geom_density()+
  labs(title="Histogram for Loan Amount") +
  xlab("Loan Amount") 

plot2 <- loan_ds %>% 
  ggplot(aes(x=LogLoanAmount)) +
  geom_histogram(bins=20) +
  geom_density()+
  labs(title="Histogram for Log Loan Amount") +
  xlab("Log Loan Amount") 

grid.arrange(plot1,plot2,ncol=2)
```


As for `ApplicantIncome`, we also have a pretty severe positive skew, so we will perform a log transformation. The data looks much better. 
```{r,class.source = "fold-show"} 
loan_ds$LogApplicantIncome <- log(loan_ds$ApplicantIncome)
```

```{r}
plot1 <- loan_ds %>% 
  ggplot(aes(x=ApplicantIncome)) +
  geom_histogram(bins=20) +
  geom_density()+
  labs(title="Histogram for Applicant Income") +
  xlab("Applicant Income") 

plot2 <- loan_ds %>% 
  ggplot(aes(x=LogApplicantIncome)) +
  geom_histogram(bins=20) +
  geom_density()+
  labs(title="Histogram for Log Applicant Income") +
  xlab("Log Applicant Income") 

grid.arrange(plot1,plot2,ncol=2)
```


Now, we will remove the original variables from our dataset
```{r,class.source = "fold-show"} 
loan_ds <- select(loan_ds,-LoanAmount)  # remove original variable
loan_ds <- select(loan_ds,-ApplicantIncome)  # remove original variable 
```






# Setting Up Models

Now that we have a better idea of how the variables in our dataset impact loan status, it's time to set up our models. We will perform our train/test split, create our recipe, then establish 10-fold cross-validation to help with our models. 


## Train/test split

Before we do any modeling, we will need to (randomly) split our dataset into training / testing data. The reason why we split our data is to avoid overfitting; we will fit the models on the training data, then use those models to make predictions on the previously unseen testing data. The testing set is reserved to be fit only once *after* the models have "learned" from the training set. From there, we can use error metrics to evaluate each model's performance. We will use a 70/30 split since our dataset is relatively small and we want to reserve enough data for the testing set. We will set a random seed before our split so that we can replicate our results, and stratify on our response.  

```{r,class.source = "fold-show"} 
set.seed(3450)
loan_split <- initial_split(loan_ds, prop = 0.70, 
                              strata = "Loan_Status")

loan_train <- training(loan_split)
loan_test <- testing(loan_split)
loan_folds <- vfold_cv(loan_train, v = 10, strata = "Loan_Status")
```

Dimensions of our datasets:
```{r,class.source = "fold-show"} 
dim(loan_train); dim(loan_test)
```



## Building the recipe

Now that we've completed all the premilminary steps, it's time to build our recipe. Think of it as following a recipe for cut-out cookies. Because we'll be using a variety of different molds (models), each cookie will look different, but their ingredients will be the same! Inside, they're all the same flour and sugar and eggs! That's what this recipe is; a unique mix of ingredients that will be fitted to different molds. Our goal turns into finding the best mold for our particular mix. From there, fitting the best model to our test data is analogous to using a different brand of the essential ingredients (ie., the test data), shaping the dough with our best cookie mold, then putting it into the oven! 

In our recipe, we'll be using 8 out of the 11 original predictors, 2 transformed variables `LogLoanAmount` and `LogApplicantIncome`, plus a new variable `Coapplicant`. 


We'll first need to upsample the data. Recall from earlier that our response was severely imbalanced; if we train our models on an imbalanced dataset, they can accidently become better at identifying one level versus another, which is undesirable. Two solutions come to mind: upsampling or downsampling. Since we have a small dataset, `step_upsample()` is the better option. We'll use `over_ratio=1` so that are equally as many Yes's as there are No's. Because upsampling is intended to be performed on the training set alone, the default skip option is `skip=TRUE`. We'll use `skip=FALSE` to make sure that it's brought the counts to be equal and then rewrite the recipe without. 

Since the numerical values of `CoapplicantIncome` are too not related to our response, we'll transform it into a categorical variable `Coappliant` instead to indicate the presence / absence of a coapplicant. We'll then scale and center our numeric predictors, and dummy-code the nominal predictors. 

```{r,class.source = "fold-show"} 
loan_recipe <- recipe(Loan_Status~., data=loan_train) %>%
  step_upsample(Loan_Status, over_ratio = 1, skip = FALSE) %>% 
  step_mutate(Coapplicant = factor(if_else(CoapplicantIncome!=0, "Yes","No",NA))) %>%
  step_rm(CoapplicantIncome)  %>% 
    # transform coapplicant income into a factor
    # Yes if CoapplicantIncome is not 0, No otherwise. 
  step_scale(all_numeric_predictors()) %>%
  step_center(all_numeric_predictors()) %>%  # scale and center
  step_dummy(all_nominal_predictors())    # convert into factor 
```

```{r,class.source = "fold-show"} 
prep(loan_recipe) %>% bake(new_data = loan_train) %>% 
  group_by(Loan_Status) %>% 
  dplyr :: summarise(count = n())
```


Now we rewrite the recipe with `skip=TRUE`:
```{r,class.source = "fold-show"} 
loan_recipe <- recipe(Loan_Status~., data=loan_train) %>%
  step_upsample(Loan_Status, over_ratio = 1, skip = TRUE) %>% 
  step_mutate(Coapplicant = factor(if_else(CoapplicantIncome!=0, "Yes","No",NA))) %>%
  step_rm(CoapplicantIncome)  %>% 
    # transform coapplicant income into a factor
    # Yes if CoapplicantIncome is not 0, No otherwise. 
  step_scale(all_numeric_predictors()) %>%
  step_center(all_numeric_predictors()) %>%  # scale and center
  step_dummy(all_nominal_predictors())    # convert into factor 
```


We can use `prep()` to check the recipe to verify it worked. 

```{r,class.source = "fold-show"} 
prep(loan_recipe) %>% 
  bake(new_data = loan_train) %>% 
  kable() %>% 
  kable_styling(full_width = F) %>% 
  scroll_box(width = "100%", height = "200px")
```

<br> 

Notice that, by dummy-coding the nomial predictors, we've increased the number of columns in our dataset. This is because each factor has been transformed into k-1 dummy variables, with one level held out as the reference (or baseline) level. The baseline level is not visible in our dataset and assigned a value of 0. For a given predictor, if the dummy variables corresponding to every other level is 0, then we default to the baseline. For instance both `Property_Area_Urban` and `Property_Area_Semiurban` are 0, then the applicant must be from a Rural area.



## K-fold cross-validation

We will stratify on our response variable `Loan_Status` and use 10 folds to perform stratified cross validation. K-fold cross-validation divides our data into k folds of roughly equal sizes, holds out the first fold as a validation set, and fits the model on the remaining k-1 folds as if they were the training set. This is repeated k times; each time, a different fold is used as a validation set. This results in k estimates of the test MSE (or in the classification case, test error rate).

```{r,class.source = "fold-show"} 
loan_folds <- vfold_cv(loan_train, v = 10, strata = Loan_Status)
```

To save computational time, we will save the results to an RDA file; once we have the model we want, we can go and load it later with no time commitment. 

```{r,class.source = "fold-show"} 
save(loan_ds, loan_folds, loan_recipe, loan_train, loan_test, 
     file = "~/Desktop/School/PSTAT/PSTAT 131/proj-final/rda_files/loan-setup.rda")
```




# Model Building

It's time to build our models! For ease of efficiency and access, I will be building each model in a separate R file and saving my results in RDA files. The models will then be loaded below for further exploration. This allows us to streamline our analysis and save on computational time. 

For each model, we will: 

  1. Set up the model by specifying its type, engine, and mode.  
  2. Set up the workflow; add the model and defined recipe. 
  
For models requiring parameter tuning, we'll complete steps 3-5. 
  
  3. Use `grid_regular` to set up tuning grids of values for the parameters we're tuning and specify levels for each.
  4. Fit the models to our folded data via `tune_grid()`.
  5. Select the best value(s) of the parameter(s) based on `roc_auc` and finalize the workflow. 
  6. Fit the final model to entire training set. 
  7. Save results to RDA file. 
  

Afterwards, we'll load back in the saved files, collect error metrics, and analyze their individual performances.



## Error metric 

The performance metric we'll be using is `roc_auc`, which stands for area under the ROC curve. The ROC (receiver operating characteristics) curve is a popular graphic that plots true positive rate (TPR) vs. false positive rate (FPR) at various threshold settings. TPR is *sensitivity* (proportion of observations that are correctly classified), while FPR is *1-specificity* (proportion of observations that are incorrectly classified); the higher the TPR, the beter. The AUC (area under curve) is a measure of the diagostic ability of a classifier, highlighting the trade-off between sensitivity and specificity. 


```{r echo=FALSE, out.width = "100%", fig.align = "center"}
knitr::include_graphics("~/Desktop/School/PSTAT/PSTAT 131/proj-final/images/image2.jpg")
```





# Model Evaluation  

It's time to load our models back in to evaluate their results! 

```{r, class.source="show"}
load(file= "~/Desktop/School/PSTAT/PSTAT 131/proj-final/rda_files/logistic.rda")
load(file= "~/Desktop/School/PSTAT/PSTAT 131/proj-final/rda_files/knn.rda")
load(file= "~/Desktop/School/PSTAT/PSTAT 131/proj-final/rda_files/en.rda")
load(file= "~/Desktop/School/PSTAT/PSTAT 131/proj-final/rda_files/lda.rda")
load(file= "~/Desktop/School/PSTAT/PSTAT 131/proj-final/rda_files/qda.rda")
load(file= "~/Desktop/School/PSTAT/PSTAT 131/proj-final/rda_files/decision-tree.rda")
```


## Model autoplots 

Here, we will visualize the results of our tuned models. We will use the `autoplot` function to visualize the effect of varying select parameters on the performance of each model according to its impact on our metric of choice. 


### K-nearest neighbors 

For the KNN model, we had 10 different levels of `neighbors`. In general, the higher the number of `neighbors`, the greater the `roc_auc`. The `roc_auc` score of the best performing model (k=10) is approxmiately 0.71, which is pretty decent. 
```{r, class.source="show"}
autoplot(knn_tune_res)
```


### Elastic net

In our elastic net model, we tuned 2 parameters with 10 levels of each: `penalty`, the amount of regularization, and `mixture`, the proportion of lasso penalty (1 for pure lasso, 0 for pure ridge). We can see from the graph that the optimal mixture was 0 (ie., pure ridge model), lower levels of mixture resulted in higher `roc_auc` scores, and that models performed worse as `penalty` increased. 
```{r, class.source="show", warning=FALSE, message=FALSE}
autoplot(en_tune_res)
```

### Decision tree

For our decision tree model, we focused on the parameter `cost_complexity` and tuned it with 10 levels. Oftentimes decision trees can have too many splits, leading to a very complex model that is likely to overfit the data. A smaller tree with fewer splits can address this issue by yielding a simpler model (better interpretation, more bias).

The idea of cost-complexity pruning is similar to that of lasso / ridge regularization; first, we grow a very large tree, then consider a sequenced of pruned subtrees and select the one that minimizes a penalized error metric. The tuning parameter `cost_complexity` controls a trade-off between a subtree's complexity and its fit to the training data; when `cost_complexity` is 0, it's the same as the the training error rate; as `cost_complexity` increases, the tree is penalized for having too many nodes. 


We can see from the plot below that a cost-complexity of about 0.001 yields the optimal model. This indicates that our tree does not require pruning / penalization after all. Note that the parameter uses the `log10_trans()` functions by default, so all of the values in our grid are in the log10 scale. 

```{r, class.source="show"}
autoplot(dt_tune_res)
```



## Model selection

Here, we will compare the performance of each model on the training data and create visualization. I've created a tibble in order to display the estimated testing `roc_auc` scores for each fitted model. 

```{r}
log_auc <- augment(log_fit, new_data = loan_train) %>%
  roc_auc(truth = Loan_Status, .pred_Yes) %>%
  select(.estimate)

lda_auc <- augment(lda_fit, new_data = loan_train) %>%
  roc_auc(truth = Loan_Status, .pred_Yes) %>%
  select(.estimate)

qda_auc <- augment(qda_fit, new_data = loan_train) %>%
  roc_auc(truth = Loan_Status, .pred_Yes) %>%
  select(.estimate)

knn_auc <- augment(knn_final_fit, new_data = loan_train) %>%
  roc_auc(truth = Loan_Status, .pred_Yes) %>%
  select(.estimate)

en_auc <- augment(en_final_fit, new_data = loan_train) %>%
  roc_auc(truth = Loan_Status, .pred_Yes) %>%
  select(.estimate)

dt_auc <- augment(dt_final_fit, new_data = loan_train) %>%
  roc_auc(truth = Loan_Status, .pred_Yes) %>%
  select(.estimate)



roc_aucs <- c(log_auc$.estimate,
                           lda_auc$.estimate,
                           qda_auc$.estimate,
                           knn_auc$.estimate,
                           en_auc$.estimate,
                           dt_auc$.estimate)

mod_names <- c("Logistic Regression",
            "LDA",
            "QDA",
            "KNN",
            "Elastic Net",
            "Decision Tree")
```


```{r}
mod_results <- tibble(Model = mod_names,
                             ROC_AUC = roc_aucs)

mod_results <- mod_results %>% 
  dplyr::arrange(-roc_aucs)

mod_results
```

While all of our models did relatively well, the best-performing model is the KNN model with an `roc_auc` score of 0.92, with the decision tree close behind at approximately 0.87 I've created a lollipop plot below to help visualize these results. 

```{r}
lp_plot <- ggplot(mod_results, aes(x = Model, y = ROC_AUC)) + 
    geom_segment( aes(x = Model, xend = 0, y = ROC_AUC, yend = 0)) +
  geom_point( size=7, color= "black", fill = alpha("blue", 0.3), alpha=0.7, shape=21, stroke=3) +
  labs(title = "Model Results") + 
  theme_minimal()

lp_plot
```



# Results of the Best Models

Now that we've identified our best models, we can continue to further analyze their true performance. We will start with the KNN model and also analyze the performance of the decision tree and QDA model as a means of comparison. 


## KNN model 

### Performance on the folds

So, the KNN model performed the best overall, but which of value of `neighbors` yields the best performance?
```{r, class.source = "show"}
# select metrics of best knn model
knn_tune_res %>% 
  collect_metrics() %>% 
  dplyr::arrange(mean) %>% 
  slice(10)
```
KNN model #10 with 11 predictors, 10 neighbors, and a mean `roc_auc` score of 0.703 performed the best! Now that we have our best model, we can fit it to our testing data to explore its true predictive power. 


### Testing the model

The KNN model did a pretty decent job in prediction on the test set. In general, an AUC value between 0.7-0.8 is considered acceptable. Given the complex nature of our problem, I would classify this as a win. 

```{r}
knn10_roc_auc <- augment(knn_final_fit, new_data = loan_test) %>%
  roc_auc(Loan_Status, .pred_Yes)  %>%
  select(.estimate)

knn10_roc_auc 
```



### ROC curve 

A confusion matrix of the test results indicate a slightly high but acceptable Type I and Type II error probability (both ~ 10%). 
```{r}
knn_test_results <- augment(knn_final_fit, new_data = loan_test)

knn_test_results %>% 
  conf_mat(truth = Loan_Status, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
```

```{r, warning=FALSE}
knn_test_results %>% 
  roc_curve(Loan_Status, .pred_Yes) %>%
  autoplot()
```

In general, the more an ROC curve resembles the top left angle of a square, the better the AUC. While our curve is not perfect, it has the correct shape and looks pretty decent.


Here's a distribution of the predicted probabilities. 
```{r, message=FALSE}
knn_test_results %>% 
  ggplot(aes(x = .pred_Yes, fill = Loan_Status)) + 
  geom_histogram(position = "dodge") + theme_bw() +
  xlab("Probability of Yes") +
  scale_fill_manual(values = c("blue", "orange"))
```


## Decision tree

As previously mentioned, we will also explore the results of the decision tree and QDA models on our testing data. We will start with the decision tree. First, let's compute its `roc_auc` score and then create visualization as needed. 

### Testing the model

The decision tree actually performed *slightly* better than the KNN model in terms of predictive accuracy, albeit by a slight margin.

```{r}
dt_roc_auc <- augment(dt_final_fit, new_data = loan_test, type = 'prob') %>%
  roc_auc(Loan_Status, .pred_Yes) %>%
  select(.estimate)

dt_roc_auc
```


### ROC curve 

The decision tree's ROC curve illustrates that, as the false positive rate (FPR) increases, the model performs better. In fact, it actually performs great once FPR exceeds 0.5. As FPR increases, specificity falls and, as indicated by the graph, sensitivity increases. For our classifier, sacraficing specitifity to boost sensitivity is the right choice.
```{r}
dt_roc_curve <- augment(dt_final_fit, new_data = loan_test, type = 'prob') %>%
  roc_curve(Loan_Status, .pred_Yes) %>%
  autoplot()

dt_roc_curve
```



## QDA

Now, it's time to analyze our quadratic discriminant analysis (QDA) classifier. In short, it's a more advanced version of a LDA model used to find a non-linear decision boundaries between classifiers, assuming each class follows a Gaussian distribution. 


### Testing the model

To my surprise, the QDA model performed the best out of our top 3 models! Its computed `roc_auc` score is only slightly higher than that of the decision tree model. Nevertheless, a 0.01 point increase is very significant when it comes to AUC. 
```{r}
qda_roc_auc <- augment(qda_fit, new_data = loan_test, type = 'prob') %>%
  roc_auc(Loan_Status, .pred_Yes) %>%
  select(.estimate)

qda_roc_auc
```

### ROC curve

To visualize this result, let's plot a ROC curve:
```{r}
augment(qda_fit, new_data = loan_test, type = 'prob') %>%
  roc_curve(Loan_Status, .pred_Yes) %>%
  autoplot()
```

Instead of fluctuating between concavity and convexity (in the case of KNN), or looking flat then curve up after a specific FPR rate (in the case of decision tree), the QDA model's ROC curve is consistently concave and looks the best out of the 3 models considered. 


# Conclusion

To summarize, in this project, we tackled the problem of loan prediction given select demographics specified in applicant profiles. We worked with a relatively small dataset (train.csv) with a large number of feature; we tidied the data, performed exploratory analysis, and fit a number of models of varying complexity and flexibility. Through analysis, testing, and assessment, we found the KNN model to be most optimal for predicting the loan status of an applicant. However, the model was not perfect and leaves room for improvment. 

In fact, none of our models performed particularly well for this problem. This can be due to a variety of factors, such as a violation of assumptions and overfitting. None of the models considereed were particularly robust in preventing overfitting (with the exception of Ridge). The Logistic model assumes that the data is linearly seperable, and performs poorly when relationships are non-linear. It also tends to overfit in higher-dimensions. The LDA model assumes a linear decision boundary, and is also prone to error in higher dimensions. The Elastic Net (Ridge) model was efficient for variance reduction, but significantly increased bias (it was the worst fit to our training data). The QDA model, while an improvement from LDA/Logistic, was not as flexible as KNN or decision tree. For more complex decision boundaries, a non-parametric approach may be preferred. The decision tree, a simpler model, is highly variant and tends to overfit. KNN doesn't require linear separability and makes no distributional assumptions; however, it does not model relationships very well and is also prone to overfitting. 

Given that the KNN and QDA models outperformed most of our models in testing, and that models with linear decision boundaries performed poorly, we can say that the relationships in our data are likely non-linear. A potential improvement would be to consider more non-linear models or non-linear extensions to some of our models; perhaps even non-parametric approaches.

As far as our error metrics (as measured by `roc_auc`), the QDA model performed better than the KNN model on the test set, whereas the KNN model perfromed better for the train. It is however important to note that both models did not have high accuracy, likely due to the fact that neither are optimal for dimensionality reduction. A more flexible approach, such as a random forest, may be better suitable for our data. Its removal of redundant features and noise would lead to less misleading data and subsequently an improvement in model accuracy.  

It's also good to acknowledge that none of our models performed particularly poorly in the face of a complex problem. Predicting loan eligibility is no easy feat, and predictive models are undoubetly prone to nuisance factors and noise. In addition, there's bias in lending. Some attributes like an applicant's background or motive for applying cannot be quantified. With this understanding, assigning a class label to each applicant based on a select few demographics seems unfair. Instead, applicants should be assessed on a case-by-case basis. 


```{r echo=FALSE, out.width = "100%", fig.align = "center"}
knitr::include_graphics("~/Desktop/School/PSTAT/PSTAT 131/proj-final/images/img03.gif")
```



